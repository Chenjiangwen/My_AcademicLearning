# Transformer

Transformer主要由4个部分组成，它们分别为：Tokenization、Embedding、Attention、Softmax

![16940557212751694055720513.png](https://fastly.jsdelivr.net/gh/Chenjiangwen/ImageHostingService@main/pic/16940557212751694055720513.png)

## Tokenization

标记化(Tokenization)是最基本的步骤。它由一个很大的标记数据集组成，包括所有单词、标点符号等。标记化步骤获取每个单词、前缀、后缀和标点符号，并将它们发送已知标记库中。这一步，对于中文自然语言处理的过程来说，就是将文章段落进行分词，例如将句子“我爱中国。”，分词成：“我”，“爱”，“中国”，“。”。

## Embedding

一旦输入被标记化，那么就可以把单词转换成数字了。这就用到Embedding了。Embedding是任何大型语言模型(LLMs)中最重要的部分之一，「它是将文本转换为数字的桥梁」。由于人类擅长文本，而计算机擅长数字，所以这个桥越坚固，语言模型就越强大。简而言之，文本Embedding将每一段文本转换成一个数字向量(一个列表)。如果两段文本相似，那么它们对应向量中的数字应该也是彼此相似的。否则，如果两段文本不同，则它们对应向量中的数字也不同。

虽然Embedding是数值，但我们也可以用几何的思维来考虑。想象一下，有一个非常简单的Embedding，它将每个单词转换成一个2维向量。如果我们想要在2维坐标中定位每个单词。那么在这个2维坐标平面上，相似的单词就会彼此靠近，而不同的单词则彼此远离。例如，在下面的Embedding中，樱桃的坐标为[6,4]，与草莓[5,4]很接近，但与城堡[1,2]有一定距离。

![16940625202761694062519756.png](https://fastly.jsdelivr.net/gh/Chenjiangwen/ImageHostingService@main/pic/16940625202761694062519756.png)

上面说的是2维向量，那么在更高维Embedding的情况下，每个词会被转换到一个更长的向量（例如，长度为4096），那么这些词不再存在于二维平面中，而是存在于一个大的 4096 维空间中。然而，「即使在那么大的向量空间中，如果两个单词的相似，它们之间的距离也会很近。Embedding的概念仍然有意义」。

接下来，我们将词嵌入泛化为文本嵌入，即将整个句子、段落甚至更长的文本转换成一个向量。然而，在Transformers的情况下，我们将使用词嵌入，这意味着句子中的每个词都被转换成相应的向量。更具体地说，输入文本中的每个标记都将被转换成Embedding中对应的向量。例如，如果我们正在考虑的句子是“Write a story.”。标记是 ‘Write’、‘a’、‘story’ 和 ‘.’，然后这些标记中的每一个都将被发送到一个长向量，我们将会得到四个向量，如下图所示：

![16940625662741694062565707.png](https://fastly.jsdelivr.net/gh/Chenjiangwen/ImageHostingService@main/pic/16940625662741694062565707.png)

对于中文来说，其实是一个道理，就是将句子分词，然后利用Embedding将每个词转成一个比较长的向量。还是拿句子：“我爱中国。”来举例子，句子分词后为：“我”，“爱”，“中国”，“。”。，「目的就是将这四个词分别对应到4个比较长的向量中，以便后面的处理。这也就是我们常说的--词嵌入」。

## Positional encoding

通过词嵌入技术，我们将句子中的每个单词都转换成了向量，下一步就是将所有这些向量都变成一个向量来处理。将一堆向量变成一个向量的最常见方法就是进行分量相加。例如，如果向量（长度为2）为 [1,2] 和 [3,4]，则它们对应的总和为 [1+3, 2+4]，等于 [4, 6]。这样做是没有问题的，但是也有一个问题需要注意，那就是「加法是可交换的，这意味着如果你以不同的顺序添加相同的数字，你会得到相同的结果」。在这种情况下，“我不难过，我很开心”和“我不开心，我很伤心”这句话会产生相同的向量。可以发现，对于含有相同词的两个句子，词的排列顺序不一样，这两句话的意思也是不同的。因此，我们必须想出一些方法，为这两个句子提供不同的向量。有几种方法可行，我们将采用其中一种：位置编码（Positional encoding）。

位置编码包括利用数字序列（它们可以是正弦和余弦、指数等），让向量与数字序列逐一相乘。即：将第一个向量乘以序列的第一个元素，将第二个向量乘以序列的第二个元素，依此类推，直到到达句子结尾；然后我们再将这些对应的向量相加。这确保了每个句子都获得一个唯一的向量，对于具有不同顺序的相同单词的句子也会产生不同的向量。在下面的示例中，对应于单词“Write”、“a”、“story”和“.”的向量。成为携带有关其位置信息的修改向量，标记为“Write (1)”、“a (2)”、“story (3)”和“. (4)”。该位置编码对于中文词嵌入向量来说是同样的道理。

![16940626332751694062633020.png](https://fastly.jsdelivr.net/gh/Chenjiangwen/ImageHostingService@main/pic/16940626332751694062633020.png)

## Transformer block

现在我们知道我们有一个与句子相对应的唯一向量，并且这个向量携带了关于句子中所有单词及其位置的信息，我「们下一步要做的就是预测这句话中的下一个单词」。这是通过一个非常非常大的神经网络完成的，该神经网络经过精确训练以预测句子中的下一个单词。然而在训练大型神经网络的时候，为了提高模型的性能我们添加了一个非常关键的组件：Attention，它是Transformer模型的关键。下一节会重点介绍Attention，但在本节，可以暂时将其想象成一种为文本中的每个单词添加上下文的方法。

「Transformer是由很多个Transformer block组成的，对于每个Transformer block其前馈网络的每个块中都添加了Attention」。同时你也可以这么想：一个大型前馈神经网络，其目标是预测下一个单词，它由几个较小的神经网络块组成，那么每个块都会添加一个注意力组件。Transformer简略图如下所示：

![16940627042781694062703563.png](https://fastly.jsdelivr.net/gh/Chenjiangwen/ImageHostingService@main/pic/16940627042781694062703563.png)

简而言之，Attention的作用是将句子（或一段文本）中的词在词嵌入中移动得更近。这样，“Money in the bank”这句话中的“bank”就会离“money”更近一些。同样，在“The bank of the river”这句话中，“bank”这个词会被移近“river”这个词。这样，两个句子中的修饰词“bank”都会携带一些相邻词信息，将这些信息加到上下文中，模型对句子的理解就变得相对容易了。

上面就是举个简单的例子，供大家理解，「其实Transformer 模型中使用的Attention比这要强大得多，它被称为多头注意力」。在多头注意力中，使用了几种不同的嵌入来修改向量并为其添加到上下文中。多头注意力帮助语言模型在处理和生成文本时达到更高的性能。如果您想更详细地了解注意力机制，请查看这篇博文 [click](https://txt.cohere.ai/what-is-attention-in-language-models/)。

## Softmax

既然您已经知道 Transformer 由许多层 Transformer 块组成，每层都包含一个注意力层和一个前馈层，您可以将其视为一个大型神经网络，用于预测句子中的下一个单词。Transformer输出所有单词的分数，其中最高分给最有可能出现在句子中的单词。

「Transformer 的最后一步是 softmax 层，它将这些分数转化为概率（加起来为 1），其中最高分数对应于最高概率」。然后，我们可以从这些概率中抽取下一个单词。在下面的示例中，Transformer将 0.5 的最高概率提供给“Once”，将 0.3 和 0.2 的概率提供给“Somewhere”和“There”。一旦我们通过概率大小来筛选，“Once”这个词就会被选中，这就是Transformer的输出。现在怎么办呢?我们重复这一步。现在我们输入文本“Write a story. Once”。一旦进入模型，最有可能的是，输出将是“upon”的。一遍又一遍地重复这个步骤，Transformer最终会写出一个故事，比如“Once upon a time, there was a …”

<br/>

## 总结

```
    • Tokenizer：将单词转换为标记。
    • Embedding：将标记转换为多维向量
    • Positional encoding：位置编码，为文本中的单词添加顺序。
    • Transformer block：它由一个注意力块和一个前馈网络模块块组成。
    • Attention：帮助语言模型理解上下文。
    • Softmax：将分数转化为概率，以便对下一个单词进行采样。这些步骤的不断地重复，就是Transformer写出优秀文本的原因。

```
