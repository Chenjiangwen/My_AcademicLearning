### [https://distill.pub/2021/gnn-intro/](https://distill.pub/2021/gnn-intro/)

---

1、
    图神经网络 测试评估：通用的、功能性测试、鲁棒性

	数据：欧几里得、非欧几里得数据。

图神经网络 原理
是什么 做什么 怎么实现
=======================================================

评估：优化设计神经网络 提升性能、 指标方法   专用：鲁棒性的测试（。。。）？神经元覆盖

**** 算法:【 测试、优化、鲁棒性】 今年定会 指标提升就发。

---

### GNN 的三大通用框架

除了图神经网络的不同变体之外，我们还介绍了几个通用框架，旨在将不同的模型集成到一个框架中。

1. J. Gilmer 等人 (J. Gilmer et. al. 2017) 提出了==消息传递神经网络== (message passing neural network， MPNN)，统一了各种图神经网络和图卷积网络方法。
2. X. Wang 等人 (X. Wang et. al. 2017) 提出了非局部神经网络 (non-local neural network, NLNN)，它结合了几种 “self-attention” 风格的方法。
3. P. W. Battaglia 等人 (P. W. Battaglia et. al. 2018) 提出了图网络 (graph network, GN)，它统一了统一了 MPNN 和 NLNN 方法以及许多其他变体，如交互网络 (Interaction Networks)，神经物理引擎 (Neural Physics Engine)，CommNet，structure2vec，GGNN，关系网络 (Relation Network)，Deep Sets 和 Point Net。

---

### 十个方向

https://github.com/THUDM/CogDL/blob/master/gnn_papers.md

分享一篇GNN十大研究方向的总结文章。出自清华大学的分享[1]。PPT中放出了他们的开源工具：CogDL[2]，面向科研的深度图神经网络工具包，其中收录了Top 100 GNN papers[3]，笔者详细看了下，觉得整理的很不错，大部分主流的SOA图神经网络模型都有收录，推荐给大家阅读。总共涵盖了10类GNN研究方向，每种研究方向下选了10篇文章，来一起预览下这10类方向。GNN Architecture：研究GNN架构/体系，基本涵盖了GNN最经典、必读的文章，如GCN三部曲, GAT, GGNN等等。Large-scale Training：研究大规模图训练，如经典的GraphSAGE/PinSAGE中做邻居结点采样的方法。Self-supervised Learning and Pre-training：研究图自监督学习和预训练，这两年来兴起的。Oversmoothing and Deep GNNs：研究GNN的过平滑问题和深层GNN。GNN的卷积操作在层数过多时会使得每个结点几乎都能间接融合其它所有结点的信息，导致所有结点的表征趋于相同，也是近年来比较热门的研究方向。Graph Robustness：研究图的鲁棒性，比如图的对抗攻击和防御等。Explainability：研究图的可解释性。Expressiveness and Generalisability：研究图的表达能力和泛化性，和经典的图同构测试关联，也是相当热门的研究方向，如经典的GIN。Heterogeneous GNNs：研究异构图神经网络，非常热门。如HAN等。GNNs for Recommendation：研究GNN在推荐场景中的应用，这类研究工作很实用，如GCMC, SRGNN, LightGCN等。Chemistry and Biology：研究GNN在化学和生物中的应用，最出名的当属AlphaFold以及最经典的消息传递框架MPNN最早就是在化学领域的应用中提出来的。

<img src="https://picx.zhimg.com/50/v2-885f0b896bc2bf7577d062f83c872cef_720w.jpg?source=2c26e567" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="1090" data-original-token="v2-e65648aef9f4a899110e0af4bb78909c" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pica.zhimg.com/v2-885f0b896bc2bf7577d062f83c872cef_r.jpg?source=2c26e567"/>

**前馈（结构）、前向（过程）、反向（算法）的概念 **

想象一条从山顶流到山脚的河流，这就像是神经网络中的信息流动。

前馈（Feedforward）：

这就像是水流从山顶流到山脚的过程，沿途没有回流到山顶的途径。在神经网络中，这对应于信息从输入层流向输出层的过程，而不会返回到输入层。每一层的输出只依赖于当前层的输入和该层的参数（权重和偏置）。


前向传播（Forward Propagation）：

这是前馈过程中的一个步骤，信息在每一层被处理（如计算或激活）后向下一层流动。在神经网络中，这就是计算每层输出和最终预测结果的过程。

反向传播（Backpropagation）：

现在假设你在山脚发现河水的流量或质量不是你想要的，你需要找出是哪个环节出了问题。在神经网络训练中，这就相当于反向传播，是一种学习机制。一旦得到输出层的结果和实际结果之间的误差（损失），就会从输出层开始，逆流而上，沿着河流路径反向“传播”这个误差，一直到达输入层。在这个过程中，通过计算误差相对于每层权重的梯度，来更新权重和偏置，以使得预测结果更接近实际结果。
总结来说，前馈是神经网络的一种结构，信息只在一个方向流动，前向传播是在这种结构中进行预测时信息处理的过程，而反向传播是一种学习算法，用于根据预测误差来更新神经网络的权重。前馈网络可以使用反向传播算法来进行训练，但其核心结构仍然是单向的信息流动，没有反馈回路。

---

**Robustness of Graph Neural Networks (GNNs):**

- CVPR 2023: A paper titled "Adversarially Robust Neural Architecture Search for Graph Neural Networks" by Beini Xie et al. focuses on understanding GNN robustness from an architectural perspective and effectively searches for optimal adversarially robust GNNs​1​.
- ICLR 2023: The paper "SlenderGNN: ACCURATE, ROBUST AND INTERPRETABLE GNN, AND THE REASONS ..." was under review for ICLR 2023. This paper appears to deal with the robustness of GNNs in a way that also maintains accuracy and interpretability​2​.
- Another paper published at ICLR 2023, titled "GNNINTERPRETER: A PROBABILISTIC GENERATIVE M-L ...", deals with instance-level explanation of GNNs. While it focuses on explanation methods, the robustness of GNNs may also be a part of the paper, as explanation methods often involve understanding the resilience of the network to various inputs​3​.
- IEEE Conference: A generic framework for robustifying GNN known as Weighted Laplacian GNN (RWLGNN) was proposed. This method combines Weighted Graph Laplacian learning with GNN implementation​4​.
- ACM: A paper discussed the defense against adversarial attacks on graphs by proposing Pro-GNN, which is designed to jointly learn a structural graph and a robust GNN model guided by graph properties that adversarial attacks are likely to violate​5​.

These papers are from top conferences in the field like CVPR, ICLR, and various IEEE conferences, indicating that they are significant contributions to the topic of GNN robustness. For further details, you would typically access the papers directly through their respective conference proceedings or journal publications.
