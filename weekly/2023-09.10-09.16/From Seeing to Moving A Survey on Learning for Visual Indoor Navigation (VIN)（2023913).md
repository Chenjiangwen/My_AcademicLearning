[https://arxiv.org/abs/2002.11310](https://arxiv.org/abs/2002.11310)

[Submitted on 26 Feb 2020 (v1), last revised 2 Jul 2022 (this version, v3)]

题目：From Seeing to Moving: A Survey on Learning for Visual Indoor Navigation (VIN)

---

# 基于学习方法解决视觉室内导航(VIN)任务的最新进展

本文总结了基于学习方法解决视觉室内导航(VIN)任务的最新进展。

![16945952407211694595239829.png](https://fastly.jsdelivr.net/gh/Chenjiangwen/ImageHostingService@main/pic/16945952407211694595239829.png)

## 1. VIN任务概述

- 目标描述
  - 标签指示
  - 图像指示
  - 语言指示
- 方法角度: 表征学习、决策过程建模
- 泛化能力: 仿真与真实环境的差距、监督学习与强化学习的比较、泛化目标的误导、知识的作用

## 2. 代表性工作简介

- **监督学习方法：** 学习可泛化的特征表示，将预测的动作与可用的实际导航轨迹进行匹配。
  - 对于大多数以目标驱动的导航任务来说，最优路径是从智能体当前位置到目标位置的最短路径，其中智能体的当前位置和目标位置分别由智能体的视觉观测和目标输入指定。
  - 在VLN任务中，将视觉导航视为特征表示学习是常见的，因为VLN任务的初步挑战是视觉观测和自然语言指令的交叉模态对齐。有了VLN基准数据集R2R中提供的期望路径，许多工作致力于学习更好的VLN任务特征表示。
    |参考文献|研究内容|
    |:--:|--|
    |[Wang et al., 2019]|提出了一种跨模态匹配架构，利用局部视觉观察和全局视觉轨迹来进行语言指令的实践|
    |[Hong et al., 2020]|提出了一种语言和视觉实体关系图建模方法，用于描述场景、物体和方向线索之间的关系|
    |[Ma et al., 2019; Zhu et al., 2020; Huang et al., 2019]|作者提出了自监督辅助任务，以加速有效特征表示的学习|
- **强化学习方法:** 将VIN视为MDP, 学习策略。通过最大化用户定义的奖励来解决VIN任务。
  - MDP： Markov Decision Process；在具身整体识别任务中，最优轨迹是一种帮助智能体尽早识别被遮挡对象的轨迹。因此，研究人员将VIN问题归纳为（部分可观察）马尔可夫决策过程（MDP），并在深度强化学习范式中解决该问题。
- 提高泛化能力的探索
  |出发点|参考文献|研究内容|
  |:--:|:--:|:--|
  |额外训练步骤|[Zhu et al., 2017]|设计了场景特定的层，只需要很少的额外训练迭代次数|
  |额外训练步骤|[Wortsman et al., 2019]|采用了基于元学习的方法，通过自监督交互损失快速适应新环境。|
  |额外训练步骤|[Savinov et al., 2018; Gupta et al., 2017; Chaplot et al., 2020a]|在执行VIN任务之前需要进行一些训练步骤来构建新环境的地图。|
  |学习可泛化表示|[Ye et al., 2019]、[Mousavian et al., 2019]|语义分割和深度图可以帮助提高泛化能力。|
  |学习可泛化表示|[Yang et al., 2018]|通过将 从Visual Genome数据集学习的物体关系图嵌入到状态表示中来改善泛化能力。|
  |学习可泛化表示|[Wu et al., 2019a]、[Ye and Yang, 2021b]|采用概率图来捕捉房间布局和物体布局。该图可以看作是与基于强化学习的运动策略集成的高层规划器，以获得更好的泛化性能。|
  |数据增强|[Fried et al., 2018]|引入的Speaker-Follower模型通过使用其话者模型在抽样的新路线上创建合成指令来进行数据增强|
  |数据增强|[Tan et al., 2019]|提出了**“环境dropout”**的方法来模拟未见环境。|
  |数据增强|[Parvaneh et al., 2020]|生成**反事实环境**来应对未见场景。|
  |数据增强|[Devo et al., 2020]|将目标定位网络与导航网络分离，使得领域随机化技术可以有效地应用于仅有目标定位网络。|
  |数据增强|[Maksymets et al., 2021]|通过在任意场景位置插入家用物品的3D扫描来增加训练布局的集合。|

## 3. 关键问题讨论

- 仿真与真实环境的差距
  - 仿真平台的一个好处是可以生成最短路径，作为训练视觉导航模型的监督信号。
  - 但在真实世界中物理机器人的控制误差影响最短路径的准确性。
  - 仿真仍然与真实世界相距甚远，因为真实世界中的许多不确定性无法被捕捉和准确建模，从而阻碍了在仿真平台上取得的进展在真实世界场景中的转移。
  - 即使在真实图像构建的仿真环境中，环境仍然是静态的，远离真实环境在光线、物体布局等方面会发生变化。因此，静态仿真环境下的解决方案仍然有很大的改进空间，这使得在动态环境下探索视觉导航任务具有挑战性。
- 监督学习与强化学习的公平比较
  - 监督学习：
    - |优点|缺点|
      |--|--|
      |对于某些任务，如点目标导航，将最短路径作为真实轨迹是有意义的。|需要大量的实际注释作为监督信号。|
      |在模拟平台下， 完全忽略控制噪声时，生成最短路径很容易|在真实世界的情景或模拟世 界中模拟真实世界的不确定性的平台中，生成最短路径代价较大|
      ||对于其他任务，如EQA、VLN和EAR任务，最短路径并不总是最佳或地面真实轨迹。最佳轨迹要么无法获得，要么需要大量的劳动，使得监督学习方法不适用|
  - 强化学习：
    - |优点|缺点|
      |--|--|
      |不需要存在真实轨迹的要求，对大部分任务都适用。|样本有效性问题使学习变得极其困难，特别是当输入具有高维度。|
  - 在使用监督学习方法和强化学习方法均适用的情况下，直接比较这两种方法是不公平的。监督学习方法的评估通常在测试环境中进行，不允许进行额外的训练过程。强化学习方法通常在训练环境中或允许进一步训练的测试环境中进行评估。
- 泛化目标的误导与潜在解决方法
  - 泛化目标的误导：
    - 对于大多数需要达到目标g的VIN任务来说，目标g与观察o之间存在较大距离，无法建立联系，学习目标会使模型使用虚假特征来记忆环境地图，从而失去对新环境的泛化能力。
  - 潜在解决方法：
    - 增强数据：通过改变目标位置和修改环境地图来增强训练数据可能会有所帮助。
    - 补充额外信息来降低不确定性，例如代理收集的历史经验h [Chenet al., 2021]。
    - 明确地建模概率分布P(m|o; g)以提高VIN的泛化能力。像[Yeand Yang, 2021b]提出的通用高级环境表示。
    - 从外部知识，如人提供的指令和公共知识库中推断出概率分布P(m|o; g)。
- 整合知识指导VIN
  - 通过先验知识来。如房屋中的功能区域和物体布局。借助这些结构的知识，期望使智能体通过避免陷入无关位置来更有效地探索环境。例如：智能体不应该在厨房花费太多时间寻找沙发。
  - 利用知识来帮助执行视觉导航任务，通常是指房间和/或物体导航任务，其中房间和/或物体的语义标签被确定为导航目标。
    |参考文献|研究内容|
    |--|--|
    |[Yang et al., 2018]|从Visual Genome [Krishna et al., 2017]语料库中提取了物体之间的关系，并通过图卷积网络将这些先验知识融入到模型中|
    |[Wu et al., 2019a]|提出了贝叶斯关系记忆（BRM）架构，通过智能体在训练环境中的经验捕捉房间布局信息，用于房间导航|
    |[Ye and Yang, 2021b]|捕捉了目标空间中所有目标之间的潜在关系，以指导以目标驱动的视觉导航任务|

## 4. 总结与展望

- 本文总结
  > 任务分类
  > 
  > > 签指示目标
  > > 
  > > 图像指示
  > > 
  > > 目标和语言指示目标
  > 
  > 两种主要方法
  > 
  > > 有监督学习
  > > 
  > > 强化学习
  > 
  > VIN任务泛化能力的讨论
- 未来研究方向
  - 在模拟平台上研究VIN任务的合法性
  - 比较基于有监督学习和基于强化学习的方法的公平性
  - 提高VIN模型的泛化能力的潜力
  - 整合知识以指导解决VIN的学习过程的途径
