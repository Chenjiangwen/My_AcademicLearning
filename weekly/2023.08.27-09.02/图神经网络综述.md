# 引言

## 1.1 图神经网络面临的训练困难

### 1.1.1 图数据结构

### 1.1.2 图神经网络模型

### 1.1.3 数据规模

### 1.1.4 硬件平台

# 图神经网络

## 2.1 消息传递机制

### 2.1.1 聚合操作

#### 2.1.1.1 基于集合

#### 2.1.1.2  基于序列

### 2.1.2 更新操作

## 2.2 模型分类

### 2.2.1 图卷积网络 GCN

#### 2.2.1.1 模型简介

卷积操作来实现邻居节点聚合. 目前 GCN 模型主要分为基于谱域和基于空间域两类。

#### 2.2.1.2 面临的训练困难

整批训练的图卷积神经网络模型训练过程要求将整个邻接矩阵载入内存,存在内存不足的限制, 分批训练的图卷积神经网络模型面临邻居爆炸的问题.

### 2.2.2 图注意力网络 GAT

#### 2.2.2.1 模型简介

引入可训练的权重参数。GAT 也提供了多头注意力机制在图神经网络中的实现, 即针对同一个节点的同一个邻居有不同的权重。

#### 2.2.2.2 面临的训练困难

图注意力网络的不同点在于基于注意力权重进行加权聚合, 而典型图卷积神经网络采用标准化求和进行节点聚合 。在大规模数据训练中, 图卷积神经网络存在内存不足和邻居爆炸问题, 在图注意力网络模型中依旧存在, 并且注意力权重的计算和存储需要消耗更多的计算和内存资源。

### 2.2.3 循环图网络 GGNN(Gated Graph Neural Network)

#### 2.2.3.1 模型简介

基于 GRU （门控循环单元）提出了针对图结构的循环图神经网络模型 GGNN。 主要针对以输出状态序列为目标的任务, 常见的图神经网络模型以输出单个状态表示为目标。

#### 2.2.3.2 面临的训练困难

整批训练的循环图神经网络同样要求将整个邻接矩阵载入内存, 并且需要更大的内存

在分批训练模式中, 如公式 (15) 所示, 其演化过程涉及直接邻居的节点表示, 图数据的不规则性在一定程度上增加了冗余计算量

### 2.2.4 图自编码器网络 SDNN

#### 2.2.4.1 模型简介

自动编码器能够通过无监督学习的方式高效学习节点表示[51-52], 由编码器和解码器两个部分构成. 编码器通过多层神经网络结构将输入空间的特征 X 映射到潜在空间 Z, 解码器采用和编码器对称的神经网络结构, 将 Z 解码到输入空间, 记做 Xˆ .自编码器通过减小重构损失 (即, X 和 Xˆ 之间的差异性) 来优化网络参数

#### 2.2.4.2 面临的训练困难

基于自动编码器的图神经网络模型不需要迭代地聚合邻居节点进行特征学习, 无法捕捉节点间的高阶关联. 借助损失函数L1st(公式 (31)) 捕捉节点间的直接关联性. 在针对大规模数据的训练中, 损失函数 L1st 要求获得所有节点对的向量表示, 受到内存限制自动编码器无法一次性生成全部的节点表示, 需要分批训练获得.
大规模数据的分批训练使得不同批节点间的比较会产生大量的冗余计算. 即使采用负样本采样的形式来进行训练, 图数据的不规则性也会对冗余计算的消减提出一定的挑战.

# 采样算法

主要通过不同粒度的采样算法实现==分批训练==以应对数据大规模性在计算效率和内存开销方面带来的挑战。

## 3.1 基于节点的采样算法

针对每个节点对其邻居节点进行采样 (即, 基于节点的采样算法), 通过采样操作将图数据规则化以适应参数共享, 为大规模数据的分批训练奠定了基础。

### 3.1.1 GraphSage

GraphSage则是以优化模型参数为目标. 如图 3(b)所示, 针对目标节点, 首先在每一阶邻居节点中随机采样固定数目的节点, 然后采用每一阶对应的聚合函数进行邻居节点特征聚合, 并借助反向传播算法对聚合函数中的参数进行学习. 通过优化之后的模型能够用于表示新的数据.

### 3.1.2 PinSage

PinSage 利用节点采样构建计算图来捕捉图结构特征, 取代整批 (full-batch) 训练模型中的图拉普拉斯矩阵和特征矩阵之间的乘法运算. 提出了基于重要性的节点采样算法, 如图 3(c)所示, 利用随机游走策略评估节点的重要性, 对每个节点选择对其最重要的 K 个节点作为其采样节点, 并在其聚合过程中进行重要性加权.

### 3.1.3 VR-GCN

为保证采样算法的收敛性，基于方差消减提出采样算法VR-GCN, 证明了无论规模如何，VR-GCN都能达到局部最优。VR-GCN仅采样两个点，并利用历史激活
节点 (即, 当前批次训练中已完成计算的节点) 来减小方差. 并通过实验表明相比于已有的采样算法,VR-GCN 能够显著减小估计梯度的偏置和方差。 VR-GCN 仅考虑 2个邻居节点也可以达到相同的性能, 大大减小了模型训练的时间复杂度和内存开销。

### 3.1.4 LGCL

将图数据结构化以满足卷积操作；LGCL 采用基于子图的训练方法,基于宽度优先算法通过逐层扩张的方法生成子图.

### 小结

基于节点的采样算法总结. 针对图神经网络中直推式训练模型存在的局限性, `GraphSage` 首先提出了基于节点的采样算法, 通过随机的节点采样算法将不规则图结构数据规则化以适应归纳式任务.如图 3(b)所示, `GraphSage` 随机采样一阶和二阶邻居, 其中一阶采样 3 个节点、二阶采样 2 个节点。不同于 GraphSage 的随机采样, `PinSage` 认为不同节点的贡献有所不同, 提出了基于重要性的采样算法, 并在节点聚合中进行重要性加权. `GraphSage`、`PinSage`主要对采样策略进行了优化, 而 `VR-GCN`则更加关注采样算法的收敛性, 通过减小梯度估计的偏置和方差来提高算法收敛性. 以上采样算法基
于已有节点进行操作, `LGCL` 则是从特征粒度进行筛选重组为新的节点进行聚合.

## 3.2 基于层的采样算法

### 3.2.1 FastGCN

### 3.2.2 AS-GCN

### 3.2.3 LADIES

### 小结

FastGCN侧重于从宏观的角度, 将图卷积过程转化为基于概率分布的积分形式, 并通过层级采样来估计积分值, 避免了邻居节点爆炸问题, 但存在相邻层
之间连接稀疏以及冗余节点的问题； AS-GCN 通过在优化目标中加入显式的方差消减来保证模型的收敛性, 并通过连接跳跃策略来捕捉二阶关联性.；LADIES将相邻两层的节点构建为二分图, 并基于此进行层级重要性采样. 基于层的采样算法通过固定层级的采样节点数目, 在一定程度上缓解了基于节点的采样算法存在的邻居节点爆炸问题, 但是只考虑了相邻层之间的节点关联, 在全局节点复用方面仍然存在一定的局限性。

## 3.3 基于子图的采样算法

### 3.3.1 Cluster-GCN

### 3.3.2 GraphSAINT

# 框架优化

## 4.1 主流框架

图神经网络迭代更新的表示机制导致训练样本之间相互依赖, 使得TensorFlow、Pytorch 等典型的神经网络框架无法有效地实现模型分批训练及其高效运行.

### 4.1.1 DGL

### 4.1.2 PyG

## 4.2 优化技术

### 4.2.1 划分聚合

### 4.2.2 异步更新

### 4.2.3 负载均衡

### 4.2.4 其它技术

# 总结与展望

## 5.1 全文总结

## 5.2 算法与框架协同优化展望
